{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **DO NOT EDIT IF INSIDE course Github folder**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Architectures and concepts\n",
    "\n",
    "Part 5.1: Convolutional neural networks<br>\n",
    "Part 5.2: Recurrent neural networks<br>\n",
    "Part 5.3: Transfer learning<br>\n",
    "Part 5.4: VAEs<br>\n",
    "Part 5.5: GANs\n",
    "\n",
    "\n",
    "[**Feedback**]((https://ulfaslak.com/vent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-26T06:48:16.271739Z",
     "start_time": "2020-02-26T06:48:16.129933Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pylab as plt\n",
    "import numpy as np\n",
    "import random, sys, io\n",
    "import requests as rq\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import LSTM\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.callbacks import LambdaCallback\n",
    "from keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1: Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pen and paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get your intuition for computations on input data in CNNs fine-tuned, I have a few small quizzes for you. First, we'll consider the size of the parameter space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 5.1.1**: Imagine you have a CNN with just one convolutional layer with a single filter. All it does, is take an input image and produce an activation map. The dimensionality of the filter in your convolutional layer is $5 \\times 5 \\times 3$. How many weights (or *parameters*) are there in this model?\n",
    ">\n",
    "> *Hint*: Don't forget the bias!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the formula for computing the size of the activation map resulting from a convolution. \n",
    "If you have a filter that is $F$ wide, your input image is $W_0$ wide, you are padding the edges by\n",
    "$P$ pixels and your stride is $S$, the resulting image will have width/height:\n",
    "\n",
    "$$ W_1 = \\frac{W_0 - F + 2P}{S} + 1 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 5.1.2**: You input an image of dimensions $28 \\times 28 \\times 3$, use a padding of 2, a stride of 1,\n",
    "and then slide your $5 \\times 5 \\times 3$ filter across the image. What is the dimensionality of the resulting activation map?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 5.1.3**: Let's say you now want to use a stride of 2, instead of 1. What problem does this immediately cause?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Maxpooling* is a method used a lot in CNNs, which downsamples the size of an activation map. It is used primarily to reduce the amount of parameters and computations needed in the network, and to avoid overfitting. Here's an illustration of how it works:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img](http://cs231n.github.io/assets/cnn/maxpool.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In *Max*pooling, for each $2 \\times 2$ square in your activation map, you pick the largest value in that square. You do this independently for every depth slice in your activation map."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** In Keras, the dimension of data is a little different from what you may expect. The first index,\n",
    "indexes datapoints, the second and third are the dimensions of your images, and the last is number of channels. So if\n",
    "you have a batch of data containing 100 datapoints, each one an RGB image (so 3 channels: red, green, blue)\n",
    "with resolution $128 \\times 128$, then the dimensionality of your input data is (100, 128, 128, 3)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 5.1.4**: Given the activation map below, what is the corresponding activation map after maxpooling ($2 \\times 2$ filter, stride 2)? Run it through a Keras maxpooling layer (check out [the docs](https://keras.io/layers/pooling/)), and report the dimensionality.\n",
    ">\n",
    "> *Hint: In Keras, layers (e.g.* `MaxPooling2D` *or* `MaxPool2D`*) are classes. An instance of such a class (e.g.* `mypool = MaxPool2D()`*) acts like a function.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T19:37:21.198895Z",
     "start_time": "2019-09-30T19:37:21.107703Z"
    }
   },
   "outputs": [],
   "source": [
    "a = np.random.random(size=(10, 28, 28, 1))  # Create 10 x 28 x 28 x 1 matrix of random numbers\n",
    "activation_map = keras.backend.variable(a)  # Load it as a Tensorflow variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CNNs in Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example sake, I have implemented a single conv. layer neural network Keras below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T19:57:40.146609Z",
     "start_time": "2019-09-30T19:57:40.141327Z"
    }
   },
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Conv2D(filters=10, kernel_size=3, strides=(1, 1), padding='valid'),\n",
    "    MaxPool2D(pool_size=(2, 2), strides=2),\n",
    "    Flatten(),\n",
    "    Dense(10)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following exercise you will use the MNIST dataset again. Here is **some code that prepares** `x_train` and `x_test`, and `y_train` and `y_test` for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T20:12:46.465418Z",
     "start_time": "2019-09-30T20:12:46.027204Z"
    }
   },
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Reshape data so it has a channel dimension\n",
    "rows, cols = x_train.shape[-2:]\n",
    "x_train = x_train.reshape(x_train.shape[0], rows, cols, 1)\n",
    "x_test = x_test.reshape(x_test.shape[0], rows, cols, 1)\n",
    "\n",
    "# Convert pixel intensities to values between 0 and 1\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "    \n",
    "# Convert target vectors to one-hot encoding\n",
    "num_classes = len(set(y_train))\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 5.1.5**: Implement Nielsen's [last convolutional neural network](http://neuralnetworksanddeeplearning.com/chap6.html#exercise_683491)\n",
    "(the one with two convolutional layers and dropout), and score an accuracy higher than 98%. It doesn't have to be\n",
    "fully identical, but his solution is pretty great, so getting close is a cheap way to score a high accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2: Recurrent Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modeling text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text prediction is a good place to start when learning about RNNs, because most of us humans have a pretty well\n",
    "optimized inner model for text prediction ourselves. We can, therefore, easily assess the performance of a neural\n",
    "network in executing this task.\n",
    "\n",
    "Below is some code that loads the screenplay for Tarantino's 1994 film 'Pulp Fiction'. I recommend reading through the\n",
    "first 20 lines or so to get a feeling for the language and style used (and enjoy probably the best written screenplay\n",
    "in the history of film)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\"PULP FICTION\" -- by Quentin Tarantino & Roger Avary\n",
      "\n",
      "\n",
      "                                      \"PULP FICTION\"\n",
      "\n",
      "                                            By\n",
      "\n",
      "                             Quentin Tarantino & Roger Avary\n",
      "\n",
      "                \n",
      "\n",
      "               PULP [pulp] n.\n",
      "\n",
      "               1. A soft, moist, shapeless mass or matter.\n",
      "\n",
      "               2. A magazine or book containing lurid subject matter and \n",
      "               being characteristically printed on rough, unfinished paper.\n",
      "\n",
      "               American Heritage Dictionary: New College Edition\n",
      "\n",
      "               INT. COFFEE SHOP – MORNING\n",
      "\n",
      "               A normal Denny's, Spires-like coffee shop in Los Angeles. \n",
      "               It's about 9:00 in the morning. While the place isn't jammed, \n",
      "               there's a healthy number of people drinking coffee, munching \n",
      "               on bacon and eating eggs.\n",
      "\n",
      "               Two of these people are a YOUNG MAN and a YOUNG WOMAN. The \n",
      "               Young Man has a slight working-class English accent and, \n",
      "               like his fellow countryman, smokes cigarettes like they're \n",
      "               going out of style.\n",
      "\n",
      "               It is impossible to tell where the Young Woman is from or \n",
      "               how old she is; everything she does contradicts something \n",
      "               she did. The boy and girl sit in a booth. Their dialogue is \n",
      "               to be said in a rapid pace \"HIS GIRL FRIDAY\" fashion.\n",
      "\n",
      "                                     YOUNG MAN\n",
      "                         No, forget it, it's too risky. I'm \n",
      "                         through doin' that shit.\n",
      "\n",
      "                                     YOUNG WOMAN\n",
      "                         You always say that, the same thing \n",
      "                         every time: never again, I'm through, \n",
      "                         too dangerous.\n",
      "\n",
      "                                     YOUNG MAN\n",
      "                         I know that's what I always say. I'm \n",
      "                         always right too, but –\n",
      "\n",
      "   \n"
     ]
    }
   ],
   "source": [
    "response = rq.get(\"http://www.dailyscript.com/scripts/pulp_fiction.html\")\n",
    "text = BeautifulSoup(response.content, \"html.parser\").getText()\n",
    "print(text[:2000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 5.2.1:** What is the most used symbol in this screenplay and what accuracy would a model constantly predicting this symbol obtain? In other words, what is the \"baseline accuracy\"?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have adapted some code for text generation from [this Keras example](https://keras.io/examples/lstm_text_generation/), and inserted questions in the code (look for `Q:`) for you to answer in the exercise below.\n",
    "\n",
    "The code fits an LSTM recurrent neural network model to the `text` variable (the Pulp Fiction manuscript). Execute it and see it run. It fits over 50 epochs, so you will probably want to interrupt it (hit `Esc` and then `I` twice) before solving the next exercise though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-14T21:10:08.952688Z",
     "start_time": "2019-10-14T21:09:02.996574Z"
    }
   },
   "outputs": [],
   "source": [
    "# Q1: What is the purpose of this block? When is `char_indices` used? What about `indices_char`?\n",
    "chars = sorted(list(set(text)))\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))\n",
    "\n",
    "# Q2: What is the purpose of this block? What does the `seqlen` and `step` parameters do?\n",
    "seqlen = 40\n",
    "step = seqlen\n",
    "sentences = []\n",
    "for i in range(0, len(text) - seqlen - 1, step):\n",
    "    sentences.append(text[i: i + seqlen + 1])\n",
    "\n",
    "# Q3: What about this block? What is `x` and what is `y`? Why do they have this dimensionality?\n",
    "x = np.zeros((len(sentences), seqlen, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(sentences), seqlen, len(chars)), dtype=np.bool)\n",
    "for i, sentence in enumerate(sentences):\n",
    "    # Q3a: What happens in this loop?\n",
    "    for t, (char_in, char_out) in enumerate(zip(sentence[:-1], sentence[1:])):\n",
    "        x[i, t, char_indices[char_in]] = 1\n",
    "        y[i, t, char_indices[char_out]] = 1\n",
    "\n",
    "\n",
    "# Q4: Here we build the model. What does the `return_sequences` argument do? Why the dense layer at the end?\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(seqlen, len(chars)), return_sequences=True))\n",
    "model.add(Dense(len(chars), activation='softmax'))\n",
    "\n",
    "model.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer=RMSprop(learning_rate=0.01),\n",
    "    metrics=['categorical_crossentropy', 'accuracy']\n",
    ")\n",
    "\n",
    "def sample(preds, temperature=1.0):\n",
    "    \"\"\"Helper function to sample an index from a probability array.\"\"\"\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.exp(np.log(preds) / temperature)  # softmax\n",
    "    preds = preds / np.sum(preds)                #\n",
    "    probas = np.random.multinomial(1, preds, 1)  # sample index\n",
    "    return np.argmax(probas)                     #\n",
    "\n",
    "\n",
    "def on_epoch_end(epoch, _):\n",
    "    \"\"\"Function invoked at end of each epoch. Prints generated text.\"\"\"\n",
    "    print()\n",
    "    print('----- Generating text after Epoch: %d' % epoch)\n",
    "\n",
    "    start_index = random.randint(0, len(text) - seqlen - 1)\n",
    "    \n",
    "    # Q5: What does diversity do?\n",
    "    for diversity in [0.2, 0.5, 1.0]:\n",
    "        print('----- diversity:', diversity)\n",
    "\n",
    "        generated = ''\n",
    "        sentence = text[start_index: start_index + seqlen]\n",
    "        generated += sentence\n",
    "        print('----- Generating with seed: \"' + sentence + '\"')\n",
    "        sys.stdout.write(generated)\n",
    "\n",
    "        for i in range(400):\n",
    "            x_pred = np.zeros((1, seqlen, len(chars)))\n",
    "            for t, char in enumerate(sentence):\n",
    "                x_pred[0, t, char_indices[char]] = 1.\n",
    "            \n",
    "            # What is the dimensionality of `preds`? Why do we input `preds[0, -1]` to the `sample` function?\n",
    "            preds = model.predict(x_pred, verbose=0)\n",
    "            next_index = sample(preds[0, -1], diversity)\n",
    "            next_char = indices_char[next_index]\n",
    "\n",
    "            sentence = sentence[1:] + next_char\n",
    "\n",
    "            sys.stdout.write(next_char)\n",
    "            sys.stdout.flush()\n",
    "        print()\n",
    "\n",
    "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)\n",
    "\n",
    "model.fit(x, y,\n",
    "          batch_size=128,\n",
    "          epochs=50,\n",
    "          callbacks=[print_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 5.2.2**: Add a callback for Tensorboard, so you can log the training process. Start training the network (takes ~10 minutes on my computer). While it's running move on to the next question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 5.2.3**: Answer the questions in the code above (look for code comments starting with `Q:`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 5.2.4**: Did the network finish training? Consider the generated text across epochs.\n",
    "1. In the early batches (0-10), the generated text looks very bad. Can you explain why the low diversity generated text contains almost only the symbol \" \" (that is, spaces)?\n",
    "2. The high diversity generated text is messed up too, but in a different way. Explain how.\n",
    "3. In later batches (20-30) what do you notice is off about the low diversity generated text?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 5.2.5**: For the network trained over all 50 epochs, generate a longer piece of text\n",
    "(say 5000 symbols long). Use the sentence `text[1486:1526]` as seed (starts with 'YOUNG MAN' ends with 'No, ')\n",
    "and set diversity to 0.5.\n",
    "Describe what features of the screenplay and language in general that the network learned in only 50 epochs.\n",
    "Also describe what serious mistakes it makes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 5.2.6**: Do the same as above, but for 40 random letters (e.g. smash away on your keyboard) as seed. What happens? Can you explain why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3: Transfer learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will follow [a very nice blog post](https://machinelearningmastery.com/how-to-use-transfer-learning-when-developing-convolutional-neural-network-models/) written by Jason Brownlee of 'Machine Learning \n",
    "Mastery' for most of these exercises. In his blog post, Jason takes the reader through\n",
    "the process of using pretrained models in Keras. Below I have outlined the steps you\n",
    "will go through with reference to his blog post. I strongly recommend you read from the\n",
    "top and down to 'Models for Transfer Learning' before proceeding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading pretrained models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first practical thing we need to figure out when doing transfer learning is loading pretrained models. Keras makes this very easy by offering a number of pretrained models for image classification which can be downloaded through their [Applications API](https://keras.io/applications/#densenet). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Applications API arguments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When loading pretrained models, we will want to provide some arguments that depend on what\n",
    "we want to do with the model after loading. Below I ask you to explain, in your own words,\n",
    "what some of these parameters do. See the Application API reference on some of the models\n",
    "and the 'Models for Transfer Learning' section in Jason's bloc post for help."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 5.3.1**: In your own words, explain what the following function arguments do in\n",
    "the different model loading functions:\n",
    "1. `include_top`\n",
    "1. `weights`\n",
    "1. `input_shape`\n",
    "1. `pooling`\n",
    "1. `classes`\n",
    "1. Explain what 'global pooling' does, and why it is needed when `include_top=False`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load a model and predict an image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 5.3.2**: Following Jason's example under 'Pre-Trained Model as Classifier'\n",
    "classify [this image](https://66.media.tumblr.com/tumblr_mc46e7Zm4R1qbqngeo1_1280.jpg).\n",
    "Print not just the most likely label, but everything that `decode_predictions` returns.\n",
    ">\n",
    "> ***Important***: *Don't use VGG as he does. It's 500 MB to download, and will take too long.\n",
    "> Use one of the smaller models instead ([here](https://keras.io/applications/#documentation-for-individual-models)'s an overview of model sizes), such as DenseNet121.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adapting pretrained models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Simple feature extractor for ML prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By removing the last layer, we can turn a pretrained convolutional neural network into a\n",
    "feature extractor. We can then use it to extract features of a large number of images and\n",
    "classify those using any machine learning model. Jason describes this under 'Pre-Trained Model as Feature Extractor Preprocessor'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 5.3.3:** Extract features for every datapoint in the [fashion-mnist dataset](https://keras.io/datasets/#fashion-mnist-database-of-fashion-articles), and build a feature matrix X. Train an SVM classifier on the learned features, and report the accuracy on the test data.\n",
    ">\n",
    "> *Hint: You can import SVM from sklearn. It has a simply API, just check out some of the examples on the [documentation page](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html).*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Changing the prediction task (switching out the last layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to achieve roughly the same thing is to remove the last layer and insert a new one with a different number of outputs. Jason describes this under 'Pre-Trained Model as Feature Extractor in Model'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 5.3.4**: Do the same as above, but by following Jason's example under 'Pre-Trained Model as Feature Extractor in Mode'.\n",
    "Compare to the accuracy you got in 6.2.1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Variational Autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming you have watched [this video](https://www.youtube.com/watch?v=9zKuYvjFFS8), answer the questions below. I also throw in some questions that link to other sources, to prompt you for a deeper understanding of some of the intuition behind VAEs:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 5.4.1**: What is typically the input and output of an autoencoder? What loss function can be used?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 5.4.2**: What is the \"bottleneck\" of an autoencoder? What can it be used for?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 5.4.3**: Purely in terms of architecture, what is the difference between an autoencoder and a variational autoencoder (VAE)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 5.4.4**: Regular autoencoders are trained to minimize a loss function with no regard to how the latent space is organized. Therefore, continuity is not guaranteed and similar datapoints may not be close to each other. We can thus say that the network is overfitting, because it uses any organization of training points in this space to minimize the loss, and is, therefore, not likely to work well with unseen data. VAEs are a regularized form of autoencoders, invented to solve this problem. Importantly, they guarantee that similar points are close in the latent space. How do they achieve this?\n",
    "    > * How are datapoints represented in the VAE latent space? What is the intuition behind this?\n",
    "    > * How is the loss function different? What is the purpose of the second term (the KL divergence)?\n",
    ">\n",
    "> *Hint: Check out this [blog post](https://towardsdatascience.com/understanding-variational-autoencoders-vaes-f70510919f73) and read the section \"Intuitions about the regularisation\"*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 5.4.5**: How is the latent vector sampled from the mean and standard deviation vectors? Explain the \"reparameterization trick\" and why it is necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 5.4.6**: What is the motivation behind the disentangled VAE (or *$\\beta$-VAE*)?\n",
    "What happens is $\\beta$ is too high? What happens when it is too small?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 5.4.EXTRA**: If you are curious about why such radical generalization\n",
    "performance increases can be achieved by just including a single new hyperparameter\n",
    "in the cost function, check out [the original paper](https://openreview.net/references/pdf?id=Sy2fzU9gl)\n",
    "from Google Deep Mind. In it, under \"$\\beta$-VAE FRAMEWORK DERIVATION\" you will\n",
    "find the intuition behind this small but powerful design modification.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 5.4.7**: Give some examples of what autoencoders can be used for. Creativity allowed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5: Generative Adversarial Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming you have watched [this video](https://www.youtube.com/watch?v=dCKbRCUyop8), answer the questions below. I also throw in some questions that link to other sources, to prompt you for a deeper understanding of some of the intuition behind GANs:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 5.5.1**: Explain in your own words how the GAN works. Touch upon:\n",
    "    > * What do the generator and discriminator networks do?\n",
    "    > * What are their respective input and output?\n",
    "    > * What would the accuracy of the discriminator be, faced with a perfect generator?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 5.5.2**: What is \"progressive growing\"?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 5.5.3**: In StyleGAN, what is the purpose of the mapping network?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 5.5.4**: How do you transform one image to another using backprop and\n",
    "gradient descent? Why does this not always work that well? How is transfer learning\n",
    "used to make it work?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 5.5.5**: From [19:20](https://www.youtube.com/watch?v=dCKbRCUyop8&feature=youtu.be&t=1160),\n",
    "outline in bullets the pipeline for obtaining the latent vector for a query image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 5.5.6**: So why go through all this trouble just to find, basically, the\n",
    "point in the latent space that represents a given image? This gets explained at\n",
    "[22:39](https://youtu.be/dCKbRCUyop8?t=1359). Summarize the idea and utility of\n",
    "labeling the points in the latent space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 5.5.7**: Besides modeling faces, can you give some examples of what GANs can be used for?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
