{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Category Development, Weak Supervision and CLassification\n",
    "Today we are gonna practice the Discovery and Exploration steps involved in a Content Analysis project. \n",
    "And finally we are gonna look at the Implementation of baseline text classifiers as described in the [\"baseline_classification.ipynb\"](https://github.com/ulfaslak/sds_tddl_2020/blob/master/baseline_classification.ipynb) notebook. \n",
    "\n",
    "Overall you will learn how to\n",
    "- setup a HSBM topic model within a docker environment (HSBM is unfortunatly hard to install)\n",
    "- Practice Computer Assisted Query Building - [\"Computer-Assisted Keyword and Document Set Discovery from Unstructured Text\"](https://gking.harvard.edu/publications/computer-assisted-keyword-and-document-set-discovery-fromunstructured-text) and [\"Building a Twitter opinion lexicon from automatically-annotated tweets\"](https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=1&cad=rja&uact=8&ved=2ahUKEwi4vabi56PoAhWEjqQKHUalAzEQFjAAegQIAxAB&url=https%3A%2F%2Fwww.sciencedirect.com%2Fscience%2Farticle%2Fpii%2FS095070511630106X&usg=AOvVaw11N9i8bUc2fwbbq9vLKsLY)\n",
    "- Practice Weak Supervision techniques combining lexical appraoches with NLP parsing systems (Stanfordnlp).\n",
    "\n",
    "\n",
    "Todays exercise will use the Kaggle Toxicity Classification dataset: https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification/data\n",
    "- Follow the url. Sign in and download the zip file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load data\n",
    "import pandas as pd\n",
    "path2tox_data = 'jigsaw-unintended-bias-in-toxicity-classification/train.csv'\n",
    "df = pd.read_csv(path2tox_data)\n",
    "# subsample data to allow faster prototyping\n",
    "df = df.sample(5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>severe_toxicity</th>\n",
       "      <th>obscene</th>\n",
       "      <th>identity_attack</th>\n",
       "      <th>insult</th>\n",
       "      <th>threat</th>\n",
       "      <th>asian</th>\n",
       "      <th>atheist</th>\n",
       "      <th>...</th>\n",
       "      <th>article_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>funny</th>\n",
       "      <th>wow</th>\n",
       "      <th>sad</th>\n",
       "      <th>likes</th>\n",
       "      <th>disagree</th>\n",
       "      <th>sexual_explicit</th>\n",
       "      <th>identity_annotator_count</th>\n",
       "      <th>toxicity_annotator_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>548089</th>\n",
       "      <td>913698</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Leaving a portfolio alone, as Scott writes, is...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>164792</td>\n",
       "      <td>approved</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>351483</th>\n",
       "      <td>673469</td>\n",
       "      <td>0.4</td>\n",
       "      <td>you really can buy a liberal for 1,500 bucks.</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>155078</td>\n",
       "      <td>approved</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>775397</th>\n",
       "      <td>5068980</td>\n",
       "      <td>0.2</td>\n",
       "      <td>So you believe only the Democrats in the legis...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>322817</td>\n",
       "      <td>approved</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1732506</th>\n",
       "      <td>6246527</td>\n",
       "      <td>0.2</td>\n",
       "      <td>So much Trump failure - everyday.</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>394752</td>\n",
       "      <td>rejected</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1632746</th>\n",
       "      <td>6122058</td>\n",
       "      <td>0.3</td>\n",
       "      <td>San Francisco is one of the best cities in the...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>387566</td>\n",
       "      <td>approved</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 45 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              id  target                                       comment_text  \\\n",
       "548089    913698     0.0  Leaving a portfolio alone, as Scott writes, is...   \n",
       "351483    673469     0.4      you really can buy a liberal for 1,500 bucks.   \n",
       "775397   5068980     0.2  So you believe only the Democrats in the legis...   \n",
       "1732506  6246527     0.2                  So much Trump failure - everyday.   \n",
       "1632746  6122058     0.3  San Francisco is one of the best cities in the...   \n",
       "\n",
       "         severe_toxicity  obscene  identity_attack  insult  threat  asian  \\\n",
       "548089               0.0      0.0              0.0     0.0     0.0    NaN   \n",
       "351483               0.0      0.0              0.2     0.4     0.0    0.0   \n",
       "775397               0.0      0.0              0.0     0.2     0.0    NaN   \n",
       "1732506              0.0      0.0              0.0     0.2     0.0    NaN   \n",
       "1632746              0.0      0.0              0.4     0.3     0.0    NaN   \n",
       "\n",
       "         atheist            ...             article_id    rating  funny  wow  \\\n",
       "548089       NaN            ...                 164792  approved      0    0   \n",
       "351483       0.0            ...                 155078  approved      0    0   \n",
       "775397       NaN            ...                 322817  approved      0    0   \n",
       "1732506      NaN            ...                 394752  rejected      0    0   \n",
       "1632746      NaN            ...                 387566  approved      0    0   \n",
       "\n",
       "         sad  likes  disagree  sexual_explicit  identity_annotator_count  \\\n",
       "548089     0      5         0              0.0                         0   \n",
       "351483     0     11         1              0.0                         4   \n",
       "775397     0      0         0              0.0                         0   \n",
       "1732506    0      0         0              0.0                         0   \n",
       "1632746    1      5         7              0.0                         0   \n",
       "\n",
       "         toxicity_annotator_count  \n",
       "548089                          4  \n",
       "351483                         10  \n",
       "775397                         10  \n",
       "1732506                         5  \n",
       "1632746                        10  \n",
       "\n",
       "[5 rows x 45 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discovery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up HSBM\n",
    "HSBM is based on the Network Package graph-tool which unfortunatly is notoriously hard to install on your ordinary laptops. \n",
    "For those of you using Linux, you can try installing on your own computer using similar commands as instructed in the Google Cloud example. \n",
    "\n",
    "Instead we have two possibilties: \n",
    "1. Create a local \"server\" on your own computer using Docker.\n",
    "2. Use a Google Cloud server as introduced in Week 1 (see exercise 1 on how to setup Google Cloud)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Docker solution\n",
    "#### First Install docker (https://docs.docker.com/install/)\n",
    "#### Enable file sharing.\n",
    "Docker runs an operating system in a closed environment, so you have to create a port to share files. \n",
    "\n",
    "Go to the Docker settings: Right click the docker icon in press settings.\n",
    "Press the /Ressources tab.\n",
    "Then /File sharing tab.\n",
    "Select the drive to be shared.\n",
    "\n",
    "### Setup the Graph-tool docker image used for HSBM.\n",
    "#### pull the image.\n",
    "`docker pull tiagopeixoto/graph-tool`\n",
    "\n",
    "#### Run the image while mounting the drive to be shared. # first allow shared drives in settings of the docker app\n",
    "#### run this.\n",
    "`docker run -v c:\\:/mnt/c -p 8888:8888 -p 6006:6006  --name graphtool -it -u root -w /home/root tiagopeixoto/graph-tool bash`\n",
    "##### In this example it was a c\\: directory that I shared. c:\\  specified the path to the shared directory on the host (your computer), and /mnt/c refered to the path in the Docker container.\n",
    " \n",
    "#### Run Jupyter notebook and navigate to port localhost://8888 in your browser\n",
    "`jupyter notebook --ip 0.0.0.0 --allow-root`\n",
    "\n",
    "#### When using it the second time use the following commands, to simple start and attach to the container.\n",
    "`docker container start graphtool`\n",
    "\n",
    "`docker attach graphtool`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google Cloud Solution\n",
    "- Login to the server you set up in week 1. See these instructions for setting it up (https://course.fast.ai/start_gcp.html)\n",
    "- Run the following conda commands:\n",
    "    - ```conda config --add channels conda-forge\n",
    "conda config --add channels ostrokach-forge\n",
    "conda config --add channels pkgw-forge\n",
    "conda install gtk3 pygobject graph-tool cairo```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 7.1: Running a HSBM topic model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Open a New .ipynb and copy the following cell to download the TOPSBM tutorial created by the Authors of HSBM.\n",
    "import requests\n",
    "for filename in ['corpus.txt','titles.txt','sbmtm.py','TopSBM-tutorial.ipynb']:\n",
    "    url = 'https://raw.githubusercontent.com/martingerlach/hSBM_Topicmodel/master/%s'%filename\n",
    "    with open(filename,'w') as f:\n",
    "        f.write(requests.get(url).text)\n",
    "# Follow the Tutorial to test out the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1.extra: Run the HSBM model on the Toxicity Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sbmtm\n",
    "\n",
    "\"\"\" TO DO \"\"\"\n",
    "# prepare documents\n",
    "    # remove urls\n",
    "    # remove infrequent words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create model and graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# investigate topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 7.2: Computer-Assisted Category Development\n",
    "First you need to install the [Gensim package](https://radimrehurek.com/gensim/)\n",
    "\n",
    "The package implements a range of unsupervised text methods, including classic topic modelling like LDA, Wordembeding models like Word2Vec and GloVe,  and an implementation of one of the early Paragraph Embedder \"Doc2Cec\". The package furthermore comes with a set of pretrained models that can be used for wordbased similarity search.\n",
    "\n",
    "In this exercise you will practice using pretrained Word Embeddings for similarity search.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Load a model from the Gensim (if not installed `! conda install -c anaconda gensim`) [pretrained models](https://github.com/RaRe-Technologies/gensim-data)\n",
    "    - I suggest you use the `\"glove-wiki-gigaword-300\"` model.\n",
    "\n",
    "```## Load gensim model\n",
    "import gensim\n",
    "import gensim.downloader as api\n",
    "glove = api.load(\"glove-wiki-gigaword-300\")```\n",
    "\n",
    "- Play around with the `most_similar()` function of the model.\n",
    "    - Try defining both the `positive=` and `negative=` argument of the function. \n",
    "\n",
    "## Build a lexicon from scratch\n",
    "- Inspect 5-10 documents in the dataset to create an Initial list of positive and negative words, or come up with a topic yourself (e.g. politics).\n",
    "- Create an input function for evaluating new words.\n",
    "    - Use the model.get_similar(word) function to get candidate words.\n",
    "    - Run through the candidates.\n",
    "    - print the word to be evaluated.\n",
    "    - use the builtin method `input()` to get manual input.\n",
    "    - use the input to either save or discard candidate words.\n",
    "    - repeat process until you have >20 words.\n",
    "- Apply a lookup function to the dataset.\n",
    "    - First you tokenize the data using a tokenizer of choice. I suggest nltk.tokenize.casual.TweetTokenizer()\n",
    "    - create a function that loops through the words and counts matching with your lexicon.\n",
    "Extra: Train your own word2vec model on the Full toxicity dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Play around with the most_similar() function of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build a lexicon from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Extra: train your own word2vec model on the full toxicity dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.3: Weak supervision combining sentiment lexicon and dependency parser.\n",
    "This exercise we will create a weak supervision scheme, that combines the Vader sentiment analysis tool implemented in the nltk package, with a nlp parser to extract relationships - i.e. who is the negative sentiment directed against.\n",
    "\n",
    "First install the stanza package(`! pip install stanza`), formerly known as stanfordnlp.\n",
    "\n",
    "The package supports a [multitude of languages](https://stanfordnlp.github.io/stanza/models.html), but each model should be downloaded first using the `stanza.download()` function. Here we need to download the english one: `stanza.download('en')`.\n",
    "\n",
    "1.import nltk.sentiment and initialize the vader sentiment analyzer.\n",
    "\n",
    "2. Apply the vader sentiment analyzer extracting only the 'neg' value, and plot the results in relation to the columns: `['black','female','asian','homosexual_gay_or_lesbian']` as a barchart. The columns express the social groups involved/addressed in the comment. \n",
    "\n",
    "3. Make a column in the dataset that expresses whether 'woman' + relevant synonyms found using the Glove model, is mentioned in the text.\n",
    "\n",
    "4. Compute the precision and recall of the woman identifier in relation to the 'female' column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stanfordnlp to match adjectives and women.\n",
    "\n",
    "5. Run the nlp pipeline by initializing the parser `stanza.Pipeline('en').\n",
    "\n",
    "6. Transform the Parse object to a list of sentences using the function `nlp_to_dict`.\n",
    "    - This will return a list of sentences, which by themselves are lsit of words including the parsed properties such as Word class.  \n",
    "\n",
    "7. Now we should get acquinted with the information involved. Count the most common lemmas of each wordclass(i.e. the 'upos' property), and print the top 10 words.\n",
    "\n",
    "8. Apply the `extract_adj_noun()` that returns a dataframe of noun-adjective pairs. \n",
    "\n",
    "9. Keep only rows where \"women\" + synonmyms is found in the noun column.\n",
    "\n",
    "10. Inspect the adjectives used, and compute which are significantly more common using the *Chi Squared measure*:\n",
    "\n",
    "$Chi^2$ express the difference between the co-occurence we observe and what we would have expected to see if two events independent, relative to the latter. I.e. how many times more prevalent is the co-occurrence we observe than what we would expect if they were independent.\n",
    "\n",
    "$X^{2}=\\sum_{i, j} \\frac{\\left(O_{i j}-E_{i j}\\right)^{2}}{E_{i j}}$\n",
    "where $i$ ranges over rows of the table, $j$ ranges over columns, $O_{i j}$ is the observed value for cell ($i,j$) and $E_ij$ is the expected value.\n",
    " \n",
    "\n",
    "11. Match the adjectives to two positive and negative word lists precompilled in the NLTK package. \n",
    "```\n",
    "from nltk.corpus import opinion_lexicon\n",
    "positive_w = set(opinion_lexicon.positive())\n",
    "negative_w = set(opinion_lexicon.negative())\n",
    "```\n",
    "12. Finally wrap this into a function, that returns true if both women+synonyms is present, and a negative word.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/master/resources_1.0.0.json: 116kB [00:00, 18.5MB/s]                    \n",
      "2020-03-18 20:06:15 INFO: Downloading default packages for language: en (English)...\n",
      "Downloading http://nlp.stanford.edu/software/stanza/1.0.0/en/default.zip: 100%|██████████| 402M/402M [00:52<00:00, 7.61MB/s] \n",
      "2020-03-18 20:07:16 INFO: Finished downloading models and saved to /home/snorre/stanza_resources.\n",
      "2020-03-18 20:07:16 INFO: Loading these models for language: en (English):\n",
      "=========================\n",
      "| Processor | Package   |\n",
      "-------------------------\n",
      "| tokenize  | ewt       |\n",
      "| pos       | ewt       |\n",
      "| lemma     | ewt       |\n",
      "| depparse  | ewt       |\n",
      "| ner       | ontonotes |\n",
      "=========================\n",
      "\n",
      "2020-03-18 20:07:17 INFO: Use device: cpu\n",
      "2020-03-18 20:07:17 INFO: Loading: tokenize\n",
      "2020-03-18 20:07:17 INFO: Loading: pos\n",
      "2020-03-18 20:07:18 INFO: Loading: lemma\n",
      "2020-03-18 20:07:18 INFO: Loading: depparse\n",
      "2020-03-18 20:07:19 INFO: Loading: ner\n",
      "2020-03-18 20:07:19 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "# 1\n",
    " \n",
    "# download English model\n",
    "# initialize English neural pipeline\n",
    "# apply to df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Helper function\"\"\"\n",
    "\n",
    "def nlp_to_dict(doc,keep_sentence=True):\n",
    "    dependencies = []\n",
    "\n",
    "    if keep_sentence == True:\n",
    "        for sent in doc.sentences:\n",
    "            #keys = [i for i in dir(sent.words[0]) if '_' !=i[0] and i!='misc']\n",
    "            temp = []\n",
    "            for word in sent.words:\n",
    "                d = word.to_dict()\n",
    "                #d = {key:getattr(word,key) for key in keys}\n",
    "                d['index'] = int(d['id'])\n",
    "                temp.append(d)\n",
    "            dependencies.append(temp)\n",
    "    else:\n",
    "        max_id = 0\n",
    "        for sent in doc.sentences:\n",
    "            keys = [i for i in dir(sent.words[0]) if '_' !=i[0] and i!='misc']\n",
    "            words = sent.words\n",
    "            \n",
    "            for word in words:\n",
    "                d = word.to_dict()\n",
    "                #d = {key:getattr(word,key) for key in keys}\n",
    "                d['index'] = int(d['id'])\n",
    "                dependencies.append(d)\n",
    "    return dependencies\n",
    "\n",
    "df['nlp_parse'] = df.nlp.apply(nlp_to_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 2\n",
    "\n",
    "# apply nlp_to_dict function\n",
    "\n",
    "# 3\n",
    "\n",
    "# count lemmas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Helper functions\"\"\"\n",
    "\n",
    "def make_network(d):\n",
    "    i2d = {}\n",
    "    g = nx.Graph()\n",
    "    for num,l in enumerate(d): \n",
    "        for i in l:\n",
    "            id_ = '%d_%d'%(num,int(i['id']))\n",
    "            i2d[id_] = i\n",
    "            parent = i['head'] \n",
    "            \n",
    "            if parent==0:\n",
    "                continue\n",
    "            parent = '%d_%d'%(num,parent)\n",
    "            g.add_edge(id_,parent)\n",
    "    # make full network\n",
    "    for i in g:\n",
    "        for j in g[i]:\n",
    "            for k in g[i]:\n",
    "                g.add_edge(j,k)\n",
    "    return g,i2d\n",
    "\n",
    "\n",
    "def extract_adj_noun(d):\n",
    "    g,i2d = make_network(d)\n",
    "    temp = []\n",
    "    for i in i2d:\n",
    "        d = i2d[i]\n",
    "        type = d['upos']\n",
    "        text_a,lemma_a = d['text'],d['lemma']\n",
    "        if type=='ADJ':\n",
    "            if not i in g:\n",
    "                continue\n",
    "            for n in g[i]:\n",
    "                if i2d[n]['upos'] == 'NOUN':\n",
    "                    d = i2d[n]\n",
    "                    text,lemma = d['text'],d['lemma']\n",
    "                    temp.append({'adj_text':text_a,'adj_lemma':lemma_a,'noun_text':text,'noun_lemma':lemma})\n",
    "    return pd.DataFrame(temp)\n",
    "\n",
    "adjnoun_df = extract_adj_noun(sample.nlp_parse.values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 4: Apply the extract_adj_noun()\n",
    "\n",
    "# Use the resulting df and (5) keep only rows where \"women\" + synonmyms is found in the noun column.\n",
    "\n",
    "# 6: Inspect the adjectives used, and compute which are significantly more common using the Chi Squared measure\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7.4 Compiling the lexicons. \n",
    "- Here you should follow the instructions from the lexical_methods.ipynb notebook of compilling and downloading the lexicons. \n",
    "- Alternatively you can download the precompilled lexicon functions that the notebook builds using the link supplied on absalon. \n",
    "- download the lexical_mining.py script that wraps around all individual lexicon functions.\n",
    "- import and appy the lexical_mining script using the `.lexical_mining()` function to the toxicity_dataset.\n",
    "- Compare the different variables in relation to the Groups: `['black','female','asian','homosexual_gay_or_lesbian']` and see if the different sentiment analytical methods agree on which group is recieving most hostility.\n",
    "    - Do this by computing the mean of the different sentiment variables: `['vader_neg','afinn_afinn','liu_negative_count','hedometer_happiness']` in relation to each group. \n",
    "    - Then compare the ranking made by each sentiment variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import lexical_mining as lex\n",
    "\n",
    "# your code"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
